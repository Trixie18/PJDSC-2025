{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "416503fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import joblib\n",
    "from datetime import datetime\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, ParameterSampler\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.metrics import (accuracy_score, precision_recall_fscore_support, \n",
    "                             confusion_matrix, classification_report)\n",
    "from lightgbm import LGBMClassifier\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e4cd1277",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Loading dataset...\n",
      "‚úÖ Loaded 756,840 records from 2020-09-20 00:00:00 to 2025-10-18 23:00:00\n",
      "üîß Handling missing values...\n",
      "‚úÖ Dataset shape: (756840, 8)\n",
      "üìà Suspension distribution:\n",
      "suspension\n",
      "0    740730\n",
      "1       168\n",
      "2       216\n",
      "3      1722\n",
      "4     11964\n",
      "5      2040\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(\"üìä Loading dataset...\")\n",
    "df = pd.read_csv('metro_manila_weather_sus_data.csv', parse_dates=['date'])\n",
    "print(f\"‚úÖ Loaded {len(df):,} records from {df['date'].min()} to {df['date'].max()}\")\n",
    "\n",
    "# Handle missing values\n",
    "print(\"üîß Handling missing values...\")\n",
    "df = df.fillna({\n",
    "    'relativehumidity_2m': df['relativehumidity_2m'].median(),\n",
    "    'temperature_2m': df['temperature_2m'].median(),\n",
    "    'precipitation': 0,\n",
    "    'apparent_temperature': df['apparent_temperature'].median(),\n",
    "    'windspeed_10m': df['windspeed_10m'].median()\n",
    "})\n",
    "\n",
    "print(f\"‚úÖ Dataset shape: {df.shape}\")\n",
    "print(f\"üìà Suspension distribution:\\n{df['suspension'].value_counts().sort_index()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2f08cc24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üõ†Ô∏è Engineering features...\n",
      "  üìä Computing rolling averages...\n",
      "  üîÑ Creating lag features...\n",
      "  üèôÔ∏è Encoding cities...\n",
      "‚úÖ Feature engineering complete. Total features: 31\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nüõ†Ô∏è Engineering features...\")\n",
    "\n",
    "# Temporal features\n",
    "df['hour'] = df['date'].dt.hour\n",
    "df['day_of_week'] = df['date'].dt.dayofweek\n",
    "df['month'] = df['date'].dt.month\n",
    "df['is_weekend'] = (df['day_of_week'] >= 5).astype(int)\n",
    "df['is_rush_hour'] = df['hour'].isin([7, 8, 9, 17, 18, 19]).astype(int)\n",
    "\n",
    "# Sort by city and date for rolling features\n",
    "df = df.sort_values(['city', 'date'])\n",
    "\n",
    "# Rolling averages by city\n",
    "print(\"  üìä Computing rolling averages...\")\n",
    "for window in [3, 6, 12]:\n",
    "    df[f'precip_roll_{window}h'] = df.groupby('city')['precipitation'].transform(\n",
    "        lambda x: x.rolling(window, min_periods=1).mean()\n",
    "    )\n",
    "    df[f'wind_roll_{window}h'] = df.groupby('city')['windspeed_10m'].transform(\n",
    "        lambda x: x.rolling(window, min_periods=1).mean()\n",
    "    )\n",
    "\n",
    "# Lag features\n",
    "print(\"  üîÑ Creating lag features...\")\n",
    "for lag in [1, 2, 3]:\n",
    "    df[f'precip_lag_{lag}h'] = df.groupby('city')['precipitation'].shift(lag).fillna(0)\n",
    "    df[f'temp_lag_{lag}h'] = df.groupby('city')['temperature_2m'].shift(lag).fillna(df['temperature_2m'].median())\n",
    "\n",
    "# Peak indicators\n",
    "df['is_precip_peak'] = (df['precipitation'] > df['precip_roll_6h'] * 1.5).astype(int)\n",
    "df['is_wind_peak'] = (df['windspeed_10m'] > df['wind_roll_6h'] * 1.3).astype(int)\n",
    "\n",
    "# Temperature delta\n",
    "df['apparent_temp_delta'] = df['apparent_temperature'] - df['temperature_2m']\n",
    "\n",
    "# Categorical features\n",
    "df['precip_intensity'] = pd.cut(df['precipitation'], \n",
    "                                bins=[-1, 0, 5, 15, 50, 200],\n",
    "                                labels=[0, 1, 2, 3, 4]).astype(int)\n",
    "\n",
    "df['wind_category'] = pd.cut(df['windspeed_10m'],\n",
    "                             bins=[-1, 20, 40, 60, 100],\n",
    "                             labels=[0, 1, 2, 3]).astype(int)\n",
    "\n",
    "# Encode city\n",
    "print(\"  üèôÔ∏è Encoding cities...\")\n",
    "city_encoder = LabelEncoder()\n",
    "df['city_encoded'] = city_encoder.fit_transform(df['city'])\n",
    "\n",
    "print(f\"‚úÖ Feature engineering complete. Total features: {len(df.columns)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a9180e31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÇÔ∏è Splitting data (80-20 time-based)...\n",
      "  üéØ Encoding target variable...\n",
      "  Original classes: [0, 1, 2, 3, 4, 5]\n",
      "  Encoded classes: [0, 1, 2, 3, 4, 5]\n",
      "  üìè Scaling features...\n",
      "‚úÖ Train set: 605,472 | Test set: 151,368\n",
      "üìä Train suspension distribution (encoded):\n",
      "0    595860\n",
      "3      1224\n",
      "4      7572\n",
      "5       816\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n‚úÇÔ∏è Splitting data (80-20 time-based)...\")\n",
    "\n",
    "# Sort by date\n",
    "df = df.sort_values('date')\n",
    "\n",
    "# Select features\n",
    "feature_cols = [\n",
    "    'relativehumidity_2m', 'temperature_2m', 'precipitation', \n",
    "    'apparent_temperature', 'windspeed_10m',\n",
    "    'hour', 'day_of_week', 'month', 'is_weekend', 'is_rush_hour',\n",
    "    'precip_roll_3h', 'precip_roll_6h', 'precip_roll_12h',\n",
    "    'wind_roll_3h', 'wind_roll_6h', 'wind_roll_12h',\n",
    "    'precip_lag_1h', 'precip_lag_2h', 'precip_lag_3h',\n",
    "    'temp_lag_1h', 'temp_lag_2h', 'temp_lag_3h',\n",
    "    'is_precip_peak', 'is_wind_peak', 'apparent_temp_delta',\n",
    "    'precip_intensity', 'wind_category', 'city_encoded'\n",
    "]\n",
    "\n",
    "X = df[feature_cols].copy()\n",
    "y = df['suspension'].copy()\n",
    "\n",
    "# Ensure all columns are numeric\n",
    "for col in X.columns:\n",
    "    if X[col].dtype == 'category':\n",
    "        X[col] = X[col].astype(int)\n",
    "\n",
    "# Encode target variable\n",
    "print(\"  üéØ Encoding target variable...\")\n",
    "label_encoder = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(y)\n",
    "\n",
    "print(f\"  Original classes: {sorted(y.unique())}\")\n",
    "print(f\"  Encoded classes: {sorted(np.unique(y_encoded))}\")\n",
    "\n",
    "# Time-based split\n",
    "split_idx = int(len(X) * 0.8)\n",
    "X_train = X.iloc[:split_idx].copy()\n",
    "X_test = X.iloc[split_idx:].copy()\n",
    "y_train = y_encoded[:split_idx]\n",
    "y_test = y_encoded[split_idx:]\n",
    "\n",
    "# Scale numerical features\n",
    "print(\"  üìè Scaling features...\")\n",
    "preprocessor = StandardScaler()\n",
    "numerical_cols = ['relativehumidity_2m', 'temperature_2m', 'precipitation', \n",
    "                 'apparent_temperature', 'windspeed_10m', 'apparent_temp_delta']\n",
    "\n",
    "X_train[numerical_cols] = preprocessor.fit_transform(X_train[numerical_cols])\n",
    "X_test[numerical_cols] = preprocessor.transform(X_test[numerical_cols])\n",
    "\n",
    "print(f\"‚úÖ Train set: {len(X_train):,} | Test set: {len(X_test):,}\")\n",
    "print(f\"üìä Train suspension distribution (encoded):\\n{pd.Series(y_train).value_counts().sort_index()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4152b66f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ü§ñ Training model with class imbalance handling...\n",
      "\n",
      "üìä Class Distribution Analysis:\n",
      "   Class 0 (encoded 0): 595,860 samples (98.41%)\n",
      "   Class 3 (encoded 3): 1,224 samples (0.20%)\n",
      "   Class 4 (encoded 4): 7,572 samples (1.25%)\n",
      "   Class 5 (encoded 5): 816 samples (0.13%)\n",
      "\n",
      "‚öñÔ∏è Calculating class weights...\n",
      "Class weights (higher = more weight for minority classes):\n",
      "   Class 0: 0.25\n",
      "   Class 3: 123.67\n",
      "   Class 4: 19.99\n",
      "   Class 5: 185.50\n",
      "\n",
      "üîÑ Applying SMOTE to balance training data...\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'split'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 32\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mimblearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mover_sampling\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SMOTE\n\u001b[1;32m     31\u001b[0m smote \u001b[38;5;241m=\u001b[39m SMOTE(random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m, k_neighbors\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m)\n\u001b[0;32m---> 32\u001b[0m X_train_balanced, y_train_balanced \u001b[38;5;241m=\u001b[39m smote\u001b[38;5;241m.\u001b[39mfit_resample(X_train_lgb, y_train)\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m   Original training size: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(X_train_lgb)\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m,\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m   Balanced training size: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(X_train_balanced)\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m,\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/imblearn/base.py:208\u001b[0m, in \u001b[0;36mBaseSampler.fit_resample\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    187\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Resample the dataset.\u001b[39;00m\n\u001b[1;32m    188\u001b[0m \n\u001b[1;32m    189\u001b[0m \u001b[38;5;124;03mParameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    205\u001b[0m \u001b[38;5;124;03m    The corresponding label of `X_resampled`.\u001b[39;00m\n\u001b[1;32m    206\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    207\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m--> 208\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mfit_resample(X, y)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/imblearn/base.py:112\u001b[0m, in \u001b[0;36mSamplerMixin.fit_resample\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    106\u001b[0m X, y, binarize_y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_X_y(X, y)\n\u001b[1;32m    108\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msampling_strategy_ \u001b[38;5;241m=\u001b[39m check_sampling_strategy(\n\u001b[1;32m    109\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msampling_strategy, y, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampling_type\n\u001b[1;32m    110\u001b[0m )\n\u001b[0;32m--> 112\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fit_resample(X, y)\n\u001b[1;32m    114\u001b[0m y_ \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    115\u001b[0m     label_binarize(output[\u001b[38;5;241m1\u001b[39m], classes\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39munique(y)) \u001b[38;5;28;01mif\u001b[39;00m binarize_y \u001b[38;5;28;01melse\u001b[39;00m output[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m    116\u001b[0m )\n\u001b[1;32m    118\u001b[0m X_, y_ \u001b[38;5;241m=\u001b[39m arrays_transformer\u001b[38;5;241m.\u001b[39mtransform(output[\u001b[38;5;241m0\u001b[39m], y_)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/imblearn/over_sampling/_smote/base.py:389\u001b[0m, in \u001b[0;36mSMOTE._fit_resample\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    386\u001b[0m X_class \u001b[38;5;241m=\u001b[39m _safe_indexing(X, target_class_indices)\n\u001b[1;32m    388\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnn_k_\u001b[38;5;241m.\u001b[39mfit(X_class)\n\u001b[0;32m--> 389\u001b[0m nns \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnn_k_\u001b[38;5;241m.\u001b[39mkneighbors(X_class, return_distance\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)[:, \u001b[38;5;241m1\u001b[39m:]\n\u001b[1;32m    390\u001b[0m X_new, y_new \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_samples(\n\u001b[1;32m    391\u001b[0m     X_class, y\u001b[38;5;241m.\u001b[39mdtype, class_sample, X_class, nns, n_samples, \u001b[38;5;241m1.0\u001b[39m\n\u001b[1;32m    392\u001b[0m )\n\u001b[1;32m    393\u001b[0m X_resampled\u001b[38;5;241m.\u001b[39mappend(X_new)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/sklearn/neighbors/_base.py:850\u001b[0m, in \u001b[0;36mKNeighborsMixin.kneighbors\u001b[0;34m(self, X, n_neighbors, return_distance)\u001b[0m\n\u001b[1;32m    843\u001b[0m use_pairwise_distances_reductions \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    844\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fit_method \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbrute\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    845\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m ArgKmin\u001b[38;5;241m.\u001b[39mis_usable_for(\n\u001b[1;32m    846\u001b[0m         X \u001b[38;5;28;01mif\u001b[39;00m X \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fit_X, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fit_X, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39meffective_metric_\n\u001b[1;32m    847\u001b[0m     )\n\u001b[1;32m    848\u001b[0m )\n\u001b[1;32m    849\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_pairwise_distances_reductions:\n\u001b[0;32m--> 850\u001b[0m     results \u001b[38;5;241m=\u001b[39m ArgKmin\u001b[38;5;241m.\u001b[39mcompute(\n\u001b[1;32m    851\u001b[0m         X\u001b[38;5;241m=\u001b[39mX,\n\u001b[1;32m    852\u001b[0m         Y\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fit_X,\n\u001b[1;32m    853\u001b[0m         k\u001b[38;5;241m=\u001b[39mn_neighbors,\n\u001b[1;32m    854\u001b[0m         metric\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39meffective_metric_,\n\u001b[1;32m    855\u001b[0m         metric_kwargs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39meffective_metric_params_,\n\u001b[1;32m    856\u001b[0m         strategy\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    857\u001b[0m         return_distance\u001b[38;5;241m=\u001b[39mreturn_distance,\n\u001b[1;32m    858\u001b[0m     )\n\u001b[1;32m    860\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m (\n\u001b[1;32m    861\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fit_method \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbrute\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmetric \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprecomputed\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m issparse(X)\n\u001b[1;32m    862\u001b[0m ):\n\u001b[1;32m    863\u001b[0m     results \u001b[38;5;241m=\u001b[39m _kneighbors_from_graph(\n\u001b[1;32m    864\u001b[0m         X, n_neighbors\u001b[38;5;241m=\u001b[39mn_neighbors, return_distance\u001b[38;5;241m=\u001b[39mreturn_distance\n\u001b[1;32m    865\u001b[0m     )\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/sklearn/metrics/_pairwise_distances_reduction/_dispatcher.py:278\u001b[0m, in \u001b[0;36mArgKmin.compute\u001b[0;34m(cls, X, Y, k, metric, chunk_size, metric_kwargs, strategy, return_distance)\u001b[0m\n\u001b[1;32m    197\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Compute the argkmin reduction.\u001b[39;00m\n\u001b[1;32m    198\u001b[0m \n\u001b[1;32m    199\u001b[0m \u001b[38;5;124;03mParameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    275\u001b[0m \u001b[38;5;124;03mreturns.\u001b[39;00m\n\u001b[1;32m    276\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    277\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m X\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m Y\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m np\u001b[38;5;241m.\u001b[39mfloat64:\n\u001b[0;32m--> 278\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ArgKmin64\u001b[38;5;241m.\u001b[39mcompute(\n\u001b[1;32m    279\u001b[0m         X\u001b[38;5;241m=\u001b[39mX,\n\u001b[1;32m    280\u001b[0m         Y\u001b[38;5;241m=\u001b[39mY,\n\u001b[1;32m    281\u001b[0m         k\u001b[38;5;241m=\u001b[39mk,\n\u001b[1;32m    282\u001b[0m         metric\u001b[38;5;241m=\u001b[39mmetric,\n\u001b[1;32m    283\u001b[0m         chunk_size\u001b[38;5;241m=\u001b[39mchunk_size,\n\u001b[1;32m    284\u001b[0m         metric_kwargs\u001b[38;5;241m=\u001b[39mmetric_kwargs,\n\u001b[1;32m    285\u001b[0m         strategy\u001b[38;5;241m=\u001b[39mstrategy,\n\u001b[1;32m    286\u001b[0m         return_distance\u001b[38;5;241m=\u001b[39mreturn_distance,\n\u001b[1;32m    287\u001b[0m     )\n\u001b[1;32m    289\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m X\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m Y\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m np\u001b[38;5;241m.\u001b[39mfloat32:\n\u001b[1;32m    290\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ArgKmin32\u001b[38;5;241m.\u001b[39mcompute(\n\u001b[1;32m    291\u001b[0m         X\u001b[38;5;241m=\u001b[39mX,\n\u001b[1;32m    292\u001b[0m         Y\u001b[38;5;241m=\u001b[39mY,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    298\u001b[0m         return_distance\u001b[38;5;241m=\u001b[39mreturn_distance,\n\u001b[1;32m    299\u001b[0m     )\n",
      "File \u001b[0;32msklearn/metrics/_pairwise_distances_reduction/_argkmin.pyx:90\u001b[0m, in \u001b[0;36msklearn.metrics._pairwise_distances_reduction._argkmin.ArgKmin64.compute\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/sklearn/utils/fixes.py:94\u001b[0m, in \u001b[0;36mthreadpool_limits\u001b[0;34m(limits, user_api)\u001b[0m\n\u001b[1;32m     92\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m controller\u001b[38;5;241m.\u001b[39mlimit(limits\u001b[38;5;241m=\u001b[39mlimits, user_api\u001b[38;5;241m=\u001b[39muser_api)\n\u001b[1;32m     93\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 94\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m threadpoolctl\u001b[38;5;241m.\u001b[39mthreadpool_limits(limits\u001b[38;5;241m=\u001b[39mlimits, user_api\u001b[38;5;241m=\u001b[39muser_api)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/threadpoolctl.py:171\u001b[0m, in \u001b[0;36mthreadpool_limits.__init__\u001b[0;34m(self, limits, user_api)\u001b[0m\n\u001b[1;32m    167\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, limits\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, user_api\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    168\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_limits, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_user_api, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prefixes \u001b[38;5;241m=\u001b[39m \\\n\u001b[1;32m    169\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_params(limits, user_api)\n\u001b[0;32m--> 171\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_original_info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_threadpool_limits()\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/threadpoolctl.py:268\u001b[0m, in \u001b[0;36mthreadpool_limits._set_threadpool_limits\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_limits \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    266\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 268\u001b[0m modules \u001b[38;5;241m=\u001b[39m _ThreadpoolInfo(prefixes\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prefixes,\n\u001b[1;32m    269\u001b[0m                           user_api\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_user_api)\n\u001b[1;32m    270\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m modules:\n\u001b[1;32m    271\u001b[0m     \u001b[38;5;66;03m# self._limits is a dict {key: num_threads} where key is either\u001b[39;00m\n\u001b[1;32m    272\u001b[0m     \u001b[38;5;66;03m# a prefix or a user_api. If a module matches both, the limit\u001b[39;00m\n\u001b[1;32m    273\u001b[0m     \u001b[38;5;66;03m# corresponding to the prefix is chosed.\u001b[39;00m\n\u001b[1;32m    274\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m module\u001b[38;5;241m.\u001b[39mprefix \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_limits:\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/threadpoolctl.py:340\u001b[0m, in \u001b[0;36m_ThreadpoolInfo.__init__\u001b[0;34m(self, user_api, prefixes, modules)\u001b[0m\n\u001b[1;32m    337\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muser_api \u001b[38;5;241m=\u001b[39m [] \u001b[38;5;28;01mif\u001b[39;00m user_api \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m user_api\n\u001b[1;32m    339\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodules \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m--> 340\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_load_modules()\n\u001b[1;32m    341\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_warn_if_incompatible_openmp()\n\u001b[1;32m    342\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/threadpoolctl.py:371\u001b[0m, in \u001b[0;36m_ThreadpoolInfo._load_modules\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    369\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Loop through loaded libraries and store supported ones\"\"\"\u001b[39;00m\n\u001b[1;32m    370\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m sys\u001b[38;5;241m.\u001b[39mplatform \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdarwin\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 371\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_find_modules_with_dyld()\n\u001b[1;32m    372\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m sys\u001b[38;5;241m.\u001b[39mplatform \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwin32\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    373\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_find_modules_with_enum_process_module_ex()\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/threadpoolctl.py:428\u001b[0m, in \u001b[0;36m_ThreadpoolInfo._find_modules_with_dyld\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    425\u001b[0m filepath \u001b[38;5;241m=\u001b[39m filepath\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    427\u001b[0m \u001b[38;5;66;03m# Store the module if it is supported and selected\u001b[39;00m\n\u001b[0;32m--> 428\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_module_from_path(filepath)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/threadpoolctl.py:515\u001b[0m, in \u001b[0;36m_ThreadpoolInfo._make_module_from_path\u001b[0;34m(self, filepath)\u001b[0m\n\u001b[1;32m    513\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m prefix \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprefixes \u001b[38;5;129;01mor\u001b[39;00m user_api \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muser_api:\n\u001b[1;32m    514\u001b[0m     module_class \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mglobals\u001b[39m()[module_class]\n\u001b[0;32m--> 515\u001b[0m     module \u001b[38;5;241m=\u001b[39m module_class(filepath, prefix, user_api, internal_api)\n\u001b[1;32m    516\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodules\u001b[38;5;241m.\u001b[39mappend(module)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/threadpoolctl.py:606\u001b[0m, in \u001b[0;36m_Module.__init__\u001b[0;34m(self, filepath, prefix, user_api, internal_api)\u001b[0m\n\u001b[1;32m    604\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minternal_api \u001b[38;5;241m=\u001b[39m internal_api\n\u001b[1;32m    605\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dynlib \u001b[38;5;241m=\u001b[39m ctypes\u001b[38;5;241m.\u001b[39mCDLL(filepath, mode\u001b[38;5;241m=\u001b[39m_RTLD_NOLOAD)\n\u001b[0;32m--> 606\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mversion \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_version()\n\u001b[1;32m    607\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_threads \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_num_threads()\n\u001b[1;32m    608\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_extra_info()\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/threadpoolctl.py:646\u001b[0m, in \u001b[0;36m_OpenBLASModule.get_version\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    643\u001b[0m get_config \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dynlib, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mopenblas_get_config\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    644\u001b[0m                      \u001b[38;5;28;01mlambda\u001b[39;00m: \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    645\u001b[0m get_config\u001b[38;5;241m.\u001b[39mrestype \u001b[38;5;241m=\u001b[39m ctypes\u001b[38;5;241m.\u001b[39mc_char_p\n\u001b[0;32m--> 646\u001b[0m config \u001b[38;5;241m=\u001b[39m get_config()\u001b[38;5;241m.\u001b[39msplit()\n\u001b[1;32m    647\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m config[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOpenBLAS\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    648\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m config[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'split'"
     ]
    }
   ],
   "source": [
    "print(\"\\nü§ñ Training model with class imbalance handling...\\n\")\n",
    "\n",
    "# Convert to float for LightGBM\n",
    "X_train_lgb = X_train.astype(float)\n",
    "X_test_lgb = X_test.astype(float)\n",
    "\n",
    "# Analyze class distribution\n",
    "print(\"üìä Class Distribution Analysis:\")\n",
    "class_counts = pd.Series(y_train).value_counts().sort_index()\n",
    "total_samples = len(y_train)\n",
    "for class_idx, count in class_counts.items():\n",
    "    percentage = (count / total_samples) * 100\n",
    "    original_class = label_encoder.inverse_transform([class_idx])[0]\n",
    "    print(f\"   Class {original_class} (encoded {class_idx}): {count:,} samples ({percentage:.2f}%)\")\n",
    "\n",
    "# Calculate class weights (inverse frequency)\n",
    "print(\"\\n‚öñÔ∏è Calculating class weights...\")\n",
    "class_weights = {}\n",
    "for class_idx in np.unique(y_train):\n",
    "    class_weights[class_idx] = total_samples / (len(np.unique(y_train)) * class_counts[class_idx])\n",
    "\n",
    "print(\"Class weights (higher = more weight for minority classes):\")\n",
    "for class_idx, weight in sorted(class_weights.items()):\n",
    "    original_class = label_encoder.inverse_transform([class_idx])[0]\n",
    "    print(f\"   Class {original_class}: {weight:.2f}\")\n",
    "\n",
    "# Apply SMOTE for oversampling minority classes\n",
    "print(\"\\nüîÑ Applying SMOTE to balance training data...\")\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "smote = SMOTE(random_state=42, k_neighbors=3)\n",
    "X_train_balanced, y_train_balanced = smote.fit_resample(X_train_lgb, y_train)\n",
    "\n",
    "print(f\"   Original training size: {len(X_train_lgb):,}\")\n",
    "print(f\"   Balanced training size: {len(X_train_balanced):,}\")\n",
    "print(\"\\nBalanced class distribution:\")\n",
    "balanced_counts = pd.Series(y_train_balanced).value_counts().sort_index()\n",
    "for class_idx, count in balanced_counts.items():\n",
    "    original_class = label_encoder.inverse_transform([class_idx])[0]\n",
    "    print(f\"   Class {original_class}: {count:,} samples\")\n",
    "\n",
    "# Define best hyperparameters\n",
    "best_params = {\n",
    "    'colsample_bytree': 0.8,\n",
    "    'learning_rate': 0.05,\n",
    "    'max_depth': 8,\n",
    "    'min_child_samples': 20,\n",
    "    'n_estimators': 600,\n",
    "    'num_leaves': 50,\n",
    "    'reg_alpha': 0.5,\n",
    "    'reg_lambda': 3.0,\n",
    "    'subsample': 0.9\n",
    "}\n",
    "\n",
    "print(\"\\nüîß Training with optimized parameters...\")\n",
    "print(\"=\"*60)\n",
    "for param, value in sorted(best_params.items()):\n",
    "    print(f\"   {param}: {value}\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Train model with SMOTE-balanced data and class weights\n",
    "best_model = LGBMClassifier(\n",
    "    **best_params,\n",
    "    num_class=len(np.unique(y_train)),\n",
    "    class_weight=class_weights,\n",
    "    random_state=42,\n",
    "    verbose=-1,\n",
    "    n_jobs=-1,\n",
    "    force_row_wise=True\n",
    ")\n",
    "\n",
    "print(\"\\nüéØ Training on balanced dataset...\")\n",
    "best_model.fit(X_train_balanced, y_train_balanced)\n",
    "print(\"‚úÖ Training complete!\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d58d43d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üìä Evaluating model on imbalanced test set...\\n\")\n",
    "\n",
    "y_pred = best_model.predict(X_test_lgb)\n",
    "y_pred_proba = best_model.predict_proba(X_test_lgb)\n",
    "\n",
    "# Overall metrics\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "    y_test, y_pred, average='weighted', zero_division=0\n",
    ")\n",
    "\n",
    "# Per-class metrics (IMPORTANT for imbalanced data)\n",
    "precision_per_class, recall_per_class, f1_per_class, support_per_class = precision_recall_fscore_support(\n",
    "    y_test, y_pred, average=None, zero_division=0\n",
    ")\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"üìà PER-CLASS PERFORMANCE (Key for Imbalanced Data)\")\n",
    "print(\"=\"*60)\n",
    "for class_idx in range(len(precision_per_class)):\n",
    "    original_class = label_encoder.inverse_transform([class_idx])[0]\n",
    "    print(f\"\\nClass {original_class} (Level {original_class}):\")\n",
    "    print(f\"   Samples in test: {support_per_class[class_idx]}\")\n",
    "    print(f\"   Precision: {precision_per_class[class_idx]:.4f}\")\n",
    "    print(f\"   Recall:    {recall_per_class[class_idx]:.4f}\")\n",
    "    print(f\"   F1 Score:  {f1_per_class[class_idx]:.4f}\")\n",
    "\n",
    "# Confusion matrix with detailed breakdown\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üîç CONFUSION MATRIX ANALYSIS\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)\n",
    "\n",
    "print(\"\\nPer-class accuracy breakdown:\")\n",
    "for class_idx in range(len(cm)):\n",
    "    original_class = label_encoder.inverse_transform([class_idx])[0]\n",
    "    correct = cm[class_idx, class_idx]\n",
    "    total = cm[class_idx, :].sum()\n",
    "    if total > 0:\n",
    "        class_accuracy = correct / total\n",
    "        print(f\"   Class {original_class}: {correct}/{total} correct ({class_accuracy:.2%})\")\n",
    "    else:\n",
    "        print(f\"   Class {original_class}: No samples in test set\")\n",
    "\n",
    "# Macro vs Weighted metrics (important distinction for imbalanced data)\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üìä AGGREGATE METRICS\")\n",
    "print(\"=\"*60)\n",
    "precision_macro, recall_macro, f1_macro, _ = precision_recall_fscore_support(\n",
    "    y_test, y_pred, average='macro', zero_division=0\n",
    ")\n",
    "\n",
    "print(f\"\\nWeighted Metrics (accounts for class frequency):\")\n",
    "print(f\"   Accuracy:  {accuracy:.4f}\")\n",
    "print(f\"   Precision: {precision:.4f}\")\n",
    "print(f\"   Recall:    {recall:.4f}\")\n",
    "print(f\"   F1 Score:  {f1:.4f}\")\n",
    "\n",
    "print(f\"\\nMacro Metrics (treats all classes equally):\")\n",
    "print(f\"   Precision: {precision_macro:.4f}\")\n",
    "print(f\"   Recall:    {recall_macro:.4f}\")\n",
    "print(f\"   F1 Score:  {f1_macro:.4f}\")\n",
    "\n",
    "# Cross-validation on original training data\n",
    "print(\"\\nüîÑ Running 5-fold cross-validation on balanced data...\")\n",
    "cv_scores = cross_val_score(best_model, X_train_balanced, y_train_balanced, cv=5, scoring='accuracy')\n",
    "print(f\"   CV Score: {cv_scores.mean():.4f} (¬±{cv_scores.std():.4f})\")\n",
    "\n",
    "# Calculate balanced accuracy (better metric for imbalanced data)\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "balanced_acc = balanced_accuracy_score(y_test, y_pred)\n",
    "print(f\"\\n‚öñÔ∏è Balanced Accuracy: {balanced_acc:.4f} (accounts for class imbalance)\")\n",
    "print(\"=\"*60 + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "157776d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üìà Extracting feature importance...\")\n",
    "\n",
    "importances = best_model.feature_importances_\n",
    "feature_importance = dict(zip(feature_cols, importances.tolist()))\n",
    "feature_importance = dict(sorted(feature_importance.items(), key=lambda x: x[1], reverse=True))\n",
    "\n",
    "print(\"Top 10 features:\")\n",
    "for i, (feat, imp) in enumerate(list(feature_importance.items())[:10], 1):\n",
    "    print(f\"  {i}. {feat}: {imp:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "282c1d21",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nüéØ Calculating suspension thresholds...\")\n",
    "\n",
    "thresholds = {}\n",
    "for level in range(6):\n",
    "    level_data = df[df['suspension'] == level]\n",
    "    if len(level_data) > 0:\n",
    "        thresholds[f'level_{level}'] = {\n",
    "            'precipitation_mean': float(level_data['precipitation'].mean()),\n",
    "            'precipitation_max': float(level_data['precipitation'].max()),\n",
    "            'windspeed_mean': float(level_data['windspeed_10m'].mean()),\n",
    "            'windspeed_max': float(level_data['windspeed_10m'].max()),\n",
    "            'humidity_mean': float(level_data['relativehumidity_2m'].mean()),\n",
    "            'count': int(len(level_data))\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37443b10",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nüíæ Saving model artifacts...\\n\")\n",
    "\n",
    "# Save model and preprocessors\n",
    "joblib.dump(best_model, 'model.pkl')\n",
    "print(\"‚úÖ Saved: model.pkl\")\n",
    "\n",
    "joblib.dump(preprocessor, 'preprocessor.pkl')\n",
    "print(\"‚úÖ Saved: preprocessor.pkl\")\n",
    "\n",
    "joblib.dump(city_encoder, 'city_encoder.pkl')\n",
    "print(\"‚úÖ Saved: city_encoder.pkl\")\n",
    "\n",
    "joblib.dump(label_encoder, 'label_encoder.pkl')\n",
    "print(\"‚úÖ Saved: label_encoder.pkl\")\n",
    "\n",
    "# Save metrics\n",
    "metrics = {\n",
    "    'best_model': 'lightgbm_tuned',\n",
    "    'models': {\n",
    "        'lightgbm_tuned': {\n",
    "            'accuracy': float(accuracy),\n",
    "            'precision': float(precision),\n",
    "            'recall': float(recall),\n",
    "            'f1': float(f1),\n",
    "            'precision_per_class': precision_per_class.tolist(),\n",
    "            'recall_per_class': recall_per_class.tolist(),\n",
    "            'f1_per_class': f1_per_class.tolist(),\n",
    "            'confusion_matrix': cm.tolist(),\n",
    "            'cv_mean': float(cv_scores.mean()),\n",
    "            'cv_std': float(cv_scores.std()),\n",
    "            'best_params': best_params\n",
    "        }\n",
    "    },\n",
    "    'hyperparameters': best_params,\n",
    "    'feature_names': feature_cols,\n",
    "    'trained_date': datetime.now().isoformat(),\n",
    "    'label_mapping': {int(k): int(v) for k, v in enumerate(label_encoder.classes_)},\n",
    "    'dataset_info': {\n",
    "        'total_records': len(df),\n",
    "        'date_range': {\n",
    "            'start': df['date'].min().isoformat(),\n",
    "            'end': df['date'].max().isoformat()\n",
    "        },\n",
    "        'cities': list(city_encoder.classes_)\n",
    "    }\n",
    "}\n",
    "\n",
    "with open('metrics.json', 'w') as f:\n",
    "    json.dump(metrics, f, indent=2)\n",
    "print(\"‚úÖ Saved: metrics.json\")\n",
    "\n",
    "with open('feature_importance.json', 'w') as f:\n",
    "    json.dump(feature_importance, f, indent=2)\n",
    "print(\"‚úÖ Saved: feature_importance.json\")\n",
    "\n",
    "np.save('confusion_matrix.npy', cm)\n",
    "print(\"‚úÖ Saved: confusion_matrix.npy\")\n",
    "\n",
    "with open('thresholds.json', 'w') as f:\n",
    "    json.dump(thresholds, f, indent=2)\n",
    "print(\"‚úÖ Saved: thresholds.json\")\n",
    "\n",
    "training_stats = {\n",
    "    'model_version': '2.0',\n",
    "    'training_date': datetime.now().isoformat(),\n",
    "    'best_model': 'lightgbm_tuned',\n",
    "    'accuracy': float(accuracy),\n",
    "    'f1_score': float(f1),\n",
    "    'train_size': len(X_train),\n",
    "    'test_size': len(X_test),\n",
    "    'total_features': len(feature_cols),\n",
    "    'tuned': True,\n",
    "    'best_hyperparameters': best_params\n",
    "}\n",
    "\n",
    "with open('training_stats.json', 'w') as f:\n",
    "    json.dump(training_stats, f, indent=2)\n",
    "print(\"‚úÖ Saved: training_stats.json\")\n",
    "\n",
    "with open('model_ready.flag', 'w') as f:\n",
    "    f.write(f\"Model trained successfully at {datetime.now()}\")\n",
    "print(\"‚úÖ Saved: model_ready.flag\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21ba3613",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üìã TRAINING REPORT\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nüéØ Model: LightGBM Classifier (Hyperparameter Tuned)\")\n",
    "print(f\"üìä Dataset: {len(df):,} records\")\n",
    "print(f\"üìÖ Date Range: {df['date'].min()} to {df['date'].max()}\")\n",
    "print(f\"üèôÔ∏è Cities: {len(city_encoder.classes_)}\")\n",
    "\n",
    "print(f\"\\nüîß Best Hyperparameters:\")\n",
    "for param, value in sorted(best_params.items()):\n",
    "    print(f\"   {param}: {value}\")\n",
    "\n",
    "print(f\"\\nüìà Performance Metrics:\")\n",
    "print(f\"   Accuracy:  {accuracy:.4f}\")\n",
    "print(f\"   Precision: {precision:.4f}\")\n",
    "print(f\"   Recall:    {recall:.4f}\")\n",
    "print(f\"   F1 Score:  {f1:.4f}\")\n",
    "print(f\"\\nüîÑ Cross-Validation:\")\n",
    "print(f\"   Mean: {cv_scores.mean():.4f}\")\n",
    "print(f\"   Std:  {cv_scores.std():.4f}\")\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"‚úÖ Model training complete and artifacts saved!\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\nüöÄ Ready to run: streamlit run main.py\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f73972a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Metro Manila Suspension Prediction - Model Training (Notebook Version)\n",
    "Simplified workflow for Jupyter notebooks\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import joblib\n",
    "from datetime import datetime\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, ParameterSampler\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.metrics import (accuracy_score, precision_recall_fscore_support, \n",
    "                             confusion_matrix, classification_report)\n",
    "from lightgbm import LGBMClassifier\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# =============================================================================\n",
    "# 1. LOAD AND PREPROCESS DATA\n",
    "# =============================================================================\n",
    "\n",
    "print(\"üìä Loading dataset...\")\n",
    "df = pd.read_csv('metro_manila_weather_sus_data.csv', parse_dates=['date'])\n",
    "print(f\"‚úÖ Loaded {len(df):,} records from {df['date'].min()} to {df['date'].max()}\")\n",
    "\n",
    "# Handle missing values\n",
    "print(\"üîß Handling missing values...\")\n",
    "df = df.fillna({\n",
    "    'relativehumidity_2m': df['relativehumidity_2m'].median(),\n",
    "    'temperature_2m': df['temperature_2m'].median(),\n",
    "    'precipitation': 0,\n",
    "    'apparent_temperature': df['apparent_temperature'].median(),\n",
    "    'windspeed_10m': df['windspeed_10m'].median()\n",
    "})\n",
    "\n",
    "print(f\"‚úÖ Dataset shape: {df.shape}\")\n",
    "print(f\"üìà Suspension distribution:\\n{df['suspension'].value_counts().sort_index()}\")\n",
    "\n",
    "# =============================================================================\n",
    "# 2. FEATURE ENGINEERING\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\nüõ†Ô∏è Engineering features...\")\n",
    "\n",
    "# Temporal features\n",
    "df['hour'] = df['date'].dt.hour\n",
    "df['day_of_week'] = df['date'].dt.dayofweek\n",
    "df['month'] = df['date'].dt.month\n",
    "df['is_weekend'] = (df['day_of_week'] >= 5).astype(int)\n",
    "df['is_rush_hour'] = df['hour'].isin([7, 8, 9, 17, 18, 19]).astype(int)\n",
    "\n",
    "# Sort by city and date for rolling features\n",
    "df = df.sort_values(['city', 'date'])\n",
    "\n",
    "# Rolling averages by city\n",
    "print(\"  üìä Computing rolling averages...\")\n",
    "for window in [3, 6, 12]:\n",
    "    df[f'precip_roll_{window}h'] = df.groupby('city')['precipitation'].transform(\n",
    "        lambda x: x.rolling(window, min_periods=1).mean()\n",
    "    )\n",
    "    df[f'wind_roll_{window}h'] = df.groupby('city')['windspeed_10m'].transform(\n",
    "        lambda x: x.rolling(window, min_periods=1).mean()\n",
    "    )\n",
    "\n",
    "# Lag features\n",
    "print(\"  üîÑ Creating lag features...\")\n",
    "for lag in [1, 2, 3]:\n",
    "    df[f'precip_lag_{lag}h'] = df.groupby('city')['precipitation'].shift(lag).fillna(0)\n",
    "    df[f'temp_lag_{lag}h'] = df.groupby('city')['temperature_2m'].shift(lag).fillna(df['temperature_2m'].median())\n",
    "\n",
    "# Peak indicators\n",
    "df['is_precip_peak'] = (df['precipitation'] > df['precip_roll_6h'] * 1.5).astype(int)\n",
    "df['is_wind_peak'] = (df['windspeed_10m'] > df['wind_roll_6h'] * 1.3).astype(int)\n",
    "\n",
    "# Temperature delta\n",
    "df['apparent_temp_delta'] = df['apparent_temperature'] - df['temperature_2m']\n",
    "\n",
    "# Categorical features\n",
    "df['precip_intensity'] = pd.cut(df['precipitation'], \n",
    "                                bins=[-1, 0, 5, 15, 50, 200],\n",
    "                                labels=[0, 1, 2, 3, 4]).astype(int)\n",
    "\n",
    "df['wind_category'] = pd.cut(df['windspeed_10m'],\n",
    "                             bins=[-1, 20, 40, 60, 100],\n",
    "                             labels=[0, 1, 2, 3]).astype(int)\n",
    "\n",
    "# Encode city\n",
    "print(\"  üèôÔ∏è Encoding cities...\")\n",
    "city_encoder = LabelEncoder()\n",
    "df['city_encoded'] = city_encoder.fit_transform(df['city'])\n",
    "\n",
    "print(f\"‚úÖ Feature engineering complete. Total features: {len(df.columns)}\")\n",
    "\n",
    "# =============================================================================\n",
    "# 3. PREPARE TRAIN-TEST SPLIT\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n‚úÇÔ∏è Splitting data (80-20 time-based)...\")\n",
    "\n",
    "# Sort by date\n",
    "df = df.sort_values('date')\n",
    "\n",
    "# Select features\n",
    "feature_cols = [\n",
    "    'relativehumidity_2m', 'temperature_2m', 'precipitation', \n",
    "    'apparent_temperature', 'windspeed_10m',\n",
    "    'hour', 'day_of_week', 'month', 'is_weekend', 'is_rush_hour',\n",
    "    'precip_roll_3h', 'precip_roll_6h', 'precip_roll_12h',\n",
    "    'wind_roll_3h', 'wind_roll_6h', 'wind_roll_12h',\n",
    "    'precip_lag_1h', 'precip_lag_2h', 'precip_lag_3h',\n",
    "    'temp_lag_1h', 'temp_lag_2h', 'temp_lag_3h',\n",
    "    'is_precip_peak', 'is_wind_peak', 'apparent_temp_delta',\n",
    "    'precip_intensity', 'wind_category', 'city_encoded'\n",
    "]\n",
    "\n",
    "X = df[feature_cols].copy()\n",
    "y = df['suspension'].copy()\n",
    "\n",
    "# Ensure all columns are numeric\n",
    "for col in X.columns:\n",
    "    if X[col].dtype == 'category':\n",
    "        X[col] = X[col].astype(int)\n",
    "\n",
    "# Encode target variable\n",
    "print(\"  üéØ Encoding target variable...\")\n",
    "label_encoder = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(y)\n",
    "\n",
    "print(f\"  Original classes: {sorted(y.unique())}\")\n",
    "print(f\"  Encoded classes: {sorted(np.unique(y_encoded))}\")\n",
    "\n",
    "# Time-based split\n",
    "split_idx = int(len(X) * 0.8)\n",
    "X_train = X.iloc[:split_idx].copy()\n",
    "X_test = X.iloc[split_idx:].copy()\n",
    "y_train = y_encoded[:split_idx]\n",
    "y_test = y_encoded[split_idx:]\n",
    "\n",
    "# Scale numerical features\n",
    "print(\"  üìè Scaling features...\")\n",
    "preprocessor = StandardScaler()\n",
    "numerical_cols = ['relativehumidity_2m', 'temperature_2m', 'precipitation', \n",
    "                 'apparent_temperature', 'windspeed_10m', 'apparent_temp_delta']\n",
    "\n",
    "X_train[numerical_cols] = preprocessor.fit_transform(X_train[numerical_cols])\n",
    "X_test[numerical_cols] = preprocessor.transform(X_test[numerical_cols])\n",
    "\n",
    "print(f\"‚úÖ Train set: {len(X_train):,} | Test set: {len(X_test):,}\")\n",
    "print(f\"üìä Train suspension distribution (encoded):\\n{pd.Series(y_train).value_counts().sort_index()}\")\n",
    "\n",
    "# =============================================================================\n",
    "# 4. HANDLE CLASS IMBALANCE & TRAIN MODEL\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\nü§ñ Training model with class imbalance handling...\\n\")\n",
    "\n",
    "# Convert to float for LightGBM\n",
    "X_train_lgb = X_train.astype(float)\n",
    "X_test_lgb = X_test.astype(float)\n",
    "\n",
    "# Analyze class distribution\n",
    "print(\"üìä Class Distribution Analysis:\")\n",
    "class_counts = pd.Series(y_train).value_counts().sort_index()\n",
    "total_samples = len(y_train)\n",
    "for class_idx, count in class_counts.items():\n",
    "    percentage = (count / total_samples) * 100\n",
    "    original_class = label_encoder.inverse_transform([class_idx])[0]\n",
    "    print(f\"   Class {original_class} (encoded {class_idx}): {count:,} samples ({percentage:.2f}%)\")\n",
    "\n",
    "# Calculate class weights (inverse frequency)\n",
    "print(\"\\n‚öñÔ∏è Calculating class weights...\")\n",
    "class_weights = {}\n",
    "for class_idx in np.unique(y_train):\n",
    "    class_weights[class_idx] = total_samples / (len(np.unique(y_train)) * class_counts[class_idx])\n",
    "\n",
    "print(\"Class weights (higher = more weight for minority classes):\")\n",
    "for class_idx, weight in sorted(class_weights.items()):\n",
    "    original_class = label_encoder.inverse_transform([class_idx])[0]\n",
    "    print(f\"   Class {original_class}: {weight:.2f}\")\n",
    "\n",
    "# Apply SMOTE for oversampling minority classes\n",
    "print(\"\\nüîÑ Applying SMOTE to balance training data...\")\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "smote = SMOTE(random_state=42, k_neighbors=3)\n",
    "X_train_balanced, y_train_balanced = smote.fit_resample(X_train_lgb, y_train)\n",
    "\n",
    "print(f\"   Original training size: {len(X_train_lgb):,}\")\n",
    "print(f\"   Balanced training size: {len(X_train_balanced):,}\")\n",
    "print(\"\\nBalanced class distribution:\")\n",
    "balanced_counts = pd.Series(y_train_balanced).value_counts().sort_index()\n",
    "for class_idx, count in balanced_counts.items():\n",
    "    original_class = label_encoder.inverse_transform([class_idx])[0]\n",
    "    print(f\"   Class {original_class}: {count:,} samples\")\n",
    "\n",
    "# Define best hyperparameters\n",
    "best_params = {\n",
    "    'colsample_bytree': 0.8,\n",
    "    'learning_rate': 0.05,\n",
    "    'max_depth': 8,\n",
    "    'min_child_samples': 20,\n",
    "    'n_estimators': 600,\n",
    "    'num_leaves': 50,\n",
    "    'reg_alpha': 0.5,\n",
    "    'reg_lambda': 3.0,\n",
    "    'subsample': 0.9\n",
    "}\n",
    "\n",
    "print(\"\\nüîß Training with optimized parameters...\")\n",
    "print(\"=\"*60)\n",
    "for param, value in sorted(best_params.items()):\n",
    "    print(f\"   {param}: {value}\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Train model with SMOTE-balanced data and class weights\n",
    "best_model = LGBMClassifier(\n",
    "    **best_params,\n",
    "    num_class=len(np.unique(y_train)),\n",
    "    class_weight=class_weights,\n",
    "    random_state=42,\n",
    "    verbose=-1,\n",
    "    n_jobs=-1,\n",
    "    force_row_wise=True\n",
    ")\n",
    "\n",
    "print(\"\\nüéØ Training on balanced dataset...\")\n",
    "best_model.fit(X_train_balanced, y_train_balanced)\n",
    "print(\"‚úÖ Training complete!\\n\")\n",
    "\n",
    "# =============================================================================\n",
    "# 5. EVALUATE MODEL (WITH CLASS-SPECIFIC METRICS)\n",
    "# =============================================================================\n",
    "\n",
    "print(\"üìä Evaluating model on imbalanced test set...\\n\")\n",
    "\n",
    "y_pred = best_model.predict(X_test_lgb)\n",
    "y_pred_proba = best_model.predict_proba(X_test_lgb)\n",
    "\n",
    "# Overall metrics\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "    y_test, y_pred, average='weighted', zero_division=0\n",
    ")\n",
    "\n",
    "# Per-class metrics (IMPORTANT for imbalanced data)\n",
    "precision_per_class, recall_per_class, f1_per_class, support_per_class = precision_recall_fscore_support(\n",
    "    y_test, y_pred, average=None, zero_division=0\n",
    ")\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"üìà PER-CLASS PERFORMANCE (Key for Imbalanced Data)\")\n",
    "print(\"=\"*60)\n",
    "for class_idx in range(len(precision_per_class)):\n",
    "    original_class = label_encoder.inverse_transform([class_idx])[0]\n",
    "    print(f\"\\nClass {original_class} (Level {original_class}):\")\n",
    "    print(f\"   Samples in test: {support_per_class[class_idx]}\")\n",
    "    print(f\"   Precision: {precision_per_class[class_idx]:.4f}\")\n",
    "    print(f\"   Recall:    {recall_per_class[class_idx]:.4f}\")\n",
    "    print(f\"   F1 Score:  {f1_per_class[class_idx]:.4f}\")\n",
    "\n",
    "# Confusion matrix with detailed breakdown\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üîç CONFUSION MATRIX ANALYSIS\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)\n",
    "\n",
    "print(\"\\nPer-class accuracy breakdown:\")\n",
    "for class_idx in range(len(cm)):\n",
    "    original_class = label_encoder.inverse_transform([class_idx])[0]\n",
    "    correct = cm[class_idx, class_idx]\n",
    "    total = cm[class_idx, :].sum()\n",
    "    if total > 0:\n",
    "        class_accuracy = correct / total\n",
    "        print(f\"   Class {original_class}: {correct}/{total} correct ({class_accuracy:.2%})\")\n",
    "    else:\n",
    "        print(f\"   Class {original_class}: No samples in test set\")\n",
    "\n",
    "# Macro vs Weighted metrics (important distinction for imbalanced data)\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üìä AGGREGATE METRICS\")\n",
    "print(\"=\"*60)\n",
    "precision_macro, recall_macro, f1_macro, _ = precision_recall_fscore_support(\n",
    "    y_test, y_pred, average='macro', zero_division=0\n",
    ")\n",
    "\n",
    "print(f\"\\nWeighted Metrics (accounts for class frequency):\")\n",
    "print(f\"   Accuracy:  {accuracy:.4f}\")\n",
    "print(f\"   Precision: {precision:.4f}\")\n",
    "print(f\"   Recall:    {recall:.4f}\")\n",
    "print(f\"   F1 Score:  {f1:.4f}\")\n",
    "\n",
    "print(f\"\\nMacro Metrics (treats all classes equally):\")\n",
    "print(f\"   Precision: {precision_macro:.4f}\")\n",
    "print(f\"   Recall:    {recall_macro:.4f}\")\n",
    "print(f\"   F1 Score:  {f1_macro:.4f}\")\n",
    "\n",
    "# Cross-validation on original training data\n",
    "print(\"\\nüîÑ Running 5-fold cross-validation on balanced data...\")\n",
    "cv_scores = cross_val_score(best_model, X_train_balanced, y_train_balanced, cv=5, scoring='accuracy')\n",
    "print(f\"   CV Score: {cv_scores.mean():.4f} (¬±{cv_scores.std():.4f})\")\n",
    "\n",
    "# Calculate balanced accuracy (better metric for imbalanced data)\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "balanced_acc = balanced_accuracy_score(y_test, y_pred)\n",
    "print(f\"\\n‚öñÔ∏è Balanced Accuracy: {balanced_acc:.4f} (accounts for class imbalance)\")\n",
    "print(\"=\"*60 + \"\\n\")\n",
    "\n",
    "# =============================================================================\n",
    "# 6. FEATURE IMPORTANCE\n",
    "# =============================================================================\n",
    "\n",
    "print(\"üìà Extracting feature importance...\")\n",
    "\n",
    "importances = best_model.feature_importances_\n",
    "feature_importance = dict(zip(feature_cols, importances.tolist()))\n",
    "feature_importance = dict(sorted(feature_importance.items(), key=lambda x: x[1], reverse=True))\n",
    "\n",
    "print(\"Top 10 features:\")\n",
    "for i, (feat, imp) in enumerate(list(feature_importance.items())[:10], 1):\n",
    "    print(f\"  {i}. {feat}: {imp:.4f}\")\n",
    "\n",
    "# =============================================================================\n",
    "# 7. CALCULATE THRESHOLDS\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\nüéØ Calculating suspension thresholds...\")\n",
    "\n",
    "thresholds = {}\n",
    "for level in range(6):\n",
    "    level_data = df[df['suspension'] == level]\n",
    "    if len(level_data) > 0:\n",
    "        thresholds[f'level_{level}'] = {\n",
    "            'precipitation_mean': float(level_data['precipitation'].mean()),\n",
    "            'precipitation_max': float(level_data['precipitation'].max()),\n",
    "            'windspeed_mean': float(level_data['windspeed_10m'].mean()),\n",
    "            'windspeed_max': float(level_data['windspeed_10m'].max()),\n",
    "            'humidity_mean': float(level_data['relativehumidity_2m'].mean()),\n",
    "            'count': int(len(level_data))\n",
    "        }\n",
    "\n",
    "# =============================================================================\n",
    "# 8. SAVE ARTIFACTS\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\nüíæ Saving model artifacts...\\n\")\n",
    "\n",
    "# Save model and preprocessors\n",
    "joblib.dump(best_model, 'model.pkl')\n",
    "print(\"‚úÖ Saved: model.pkl\")\n",
    "\n",
    "joblib.dump(preprocessor, 'preprocessor.pkl')\n",
    "print(\"‚úÖ Saved: preprocessor.pkl\")\n",
    "\n",
    "joblib.dump(city_encoder, 'city_encoder.pkl')\n",
    "print(\"‚úÖ Saved: city_encoder.pkl\")\n",
    "\n",
    "joblib.dump(label_encoder, 'label_encoder.pkl')\n",
    "print(\"‚úÖ Saved: label_encoder.pkl\")\n",
    "\n",
    "# Save metrics\n",
    "metrics = {\n",
    "    'best_model': 'lightgbm_tuned',\n",
    "    'models': {\n",
    "        'lightgbm_tuned': {\n",
    "            'accuracy': float(accuracy),\n",
    "            'precision': float(precision),\n",
    "            'recall': float(recall),\n",
    "            'f1': float(f1),\n",
    "            'precision_per_class': precision_per_class.tolist(),\n",
    "            'recall_per_class': recall_per_class.tolist(),\n",
    "            'f1_per_class': f1_per_class.tolist(),\n",
    "            'confusion_matrix': cm.tolist(),\n",
    "            'cv_mean': float(cv_scores.mean()),\n",
    "            'cv_std': float(cv_scores.std()),\n",
    "            'best_params': best_params\n",
    "        }\n",
    "    },\n",
    "    'hyperparameters': best_params,\n",
    "    'feature_names': feature_cols,\n",
    "    'trained_date': datetime.now().isoformat(),\n",
    "    'label_mapping': {int(k): int(v) for k, v in enumerate(label_encoder.classes_)},\n",
    "    'dataset_info': {\n",
    "        'total_records': len(df),\n",
    "        'date_range': {\n",
    "            'start': df['date'].min().isoformat(),\n",
    "            'end': df['date'].max().isoformat()\n",
    "        },\n",
    "        'cities': list(city_encoder.classes_)\n",
    "    }\n",
    "}\n",
    "\n",
    "with open('metrics.json', 'w') as f:\n",
    "    json.dump(metrics, f, indent=2)\n",
    "print(\"‚úÖ Saved: metrics.json\")\n",
    "\n",
    "with open('feature_importance.json', 'w') as f:\n",
    "    json.dump(feature_importance, f, indent=2)\n",
    "print(\"‚úÖ Saved: feature_importance.json\")\n",
    "\n",
    "np.save('confusion_matrix.npy', cm)\n",
    "print(\"‚úÖ Saved: confusion_matrix.npy\")\n",
    "\n",
    "with open('thresholds.json', 'w') as f:\n",
    "    json.dump(thresholds, f, indent=2)\n",
    "print(\"‚úÖ Saved: thresholds.json\")\n",
    "\n",
    "training_stats = {\n",
    "    'model_version': '2.0',\n",
    "    'training_date': datetime.now().isoformat(),\n",
    "    'best_model': 'lightgbm_tuned',\n",
    "    'accuracy': float(accuracy),\n",
    "    'f1_score': float(f1),\n",
    "    'train_size': len(X_train),\n",
    "    'test_size': len(X_test),\n",
    "    'total_features': len(feature_cols),\n",
    "    'tuned': True,\n",
    "    'best_hyperparameters': best_params\n",
    "}\n",
    "\n",
    "with open('training_stats.json', 'w') as f:\n",
    "    json.dump(training_stats, f, indent=2)\n",
    "print(\"‚úÖ Saved: training_stats.json\")\n",
    "\n",
    "with open('model_ready.flag', 'w') as f:\n",
    "    f.write(f\"Model trained successfully at {datetime.now()}\")\n",
    "print(\"‚úÖ Saved: model_ready.flag\")\n",
    "\n",
    "# =============================================================================\n",
    "# 9. TRAINING REPORT\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üìã TRAINING REPORT\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nüéØ Model: LightGBM Classifier (Hyperparameter Tuned)\")\n",
    "print(f\"üìä Dataset: {len(df):,} records\")\n",
    "print(f\"üìÖ Date Range: {df['date'].min()} to {df['date'].max()}\")\n",
    "print(f\"üèôÔ∏è Cities: {len(city_encoder.classes_)}\")\n",
    "\n",
    "print(f\"\\nüîß Best Hyperparameters:\")\n",
    "for param, value in sorted(best_params.items()):\n",
    "    print(f\"   {param}: {value}\")\n",
    "\n",
    "print(f\"\\nüìà Performance Metrics:\")\n",
    "print(f\"   Accuracy:  {accuracy:.4f}\")\n",
    "print(f\"   Precision: {precision:.4f}\")\n",
    "print(f\"   Recall:    {recall:.4f}\")\n",
    "print(f\"   F1 Score:  {f1:.4f}\")\n",
    "print(f\"\\nüîÑ Cross-Validation:\")\n",
    "print(f\"   Mean: {cv_scores.mean():.4f}\")\n",
    "print(f\"   Std:  {cv_scores.std():.4f}\")\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"‚úÖ Model training complete and artifacts saved!\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\nüöÄ Ready to run: streamlit run main.py\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
